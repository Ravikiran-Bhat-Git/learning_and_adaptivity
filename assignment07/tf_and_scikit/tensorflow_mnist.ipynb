{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Using the python tools of your preference (TensorFlow, scikit-learn, numpy, pandas, etc).\n",
    "\n",
    "1.- Obtain the MNIST dataset and normalize it for use with a classifier.\n",
    "\n",
    "2.- Select two different machine learning classification models (like logistic regression, random forests, SVMs, Gaussian Mixtures, Naive Bayes, KNN, etc). Let's call them Model A and B.\n",
    "\n",
    "3.- Train both models on the MNIST dataset and achieve a \"decent\" testing accuracy (over 90%).\n",
    "\n",
    "4.- Using the gradient sign equation in Slide 27 of the lecture, generate 100 adversarial examples for each model. Analyze your results, like what confidences or scores each example obtains, and to what degree they fool the model. Remember to tune the parameter epsilon to a reasonable value (the original paper uses epsilon = 0.007)\n",
    "\n",
    "5.- Use the adversarial examples of A with model B, and the adversarial examples of B with model A. Do they fool each other?\n",
    "\n",
    "6.- With one of the models, generate 60000 new adversarial examples (same size as the training set) and create a new training set containing both the original training data and your adversarial examples. Train both models again with this new training set, and evaluate it with the original MNIST test set. Then answer the following questions:\n",
    "\n",
    "   - Does classification performance improve?\n",
    "\n",
    "   - Is the new model less or more susceptible to adversarial examples?\n",
    "\n",
    "   - Do you think you can use a regularization method in order to make the model less susceptible to adversarial examples?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "import tensorflow.contrib.eager as tfe\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set Eager API\n",
    "tfe.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#Load MNIST data from tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot images\n",
    "def plot_img(image, index):\n",
    "    f, ax = plt.subplots(1, len(index))\n",
    "    for i in range(len(index)):\n",
    "        ax[i].imshow(np.reshape(image[index[i]], (28,28)))\n",
    "    plt.show()\n",
    "def show_img(image, index):\n",
    "    plt.imshow(np.reshape(image[index], (28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot_img(mnist.train.images, [1,167,200,400,1099])\n",
    "#show_img(mnist.train.images, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow - logstic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.5\n",
    "batch_size = 128\n",
    "num_steps = 1000\n",
    "display_step = 100\n",
    "# Iterator for the dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (mnist.train.images, mnist.train.labels)).batch(batch_size)\n",
    "dataset_iter = tfe.Iterator(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "W = tfe.Variable(tf.zeros([784, 10]), name='weights')\n",
    "b = tfe.Variable(tf.zeros([10]), name='bias')\n",
    "\n",
    "#linear combiner (v)\n",
    "def linear_combiner(inputs):\n",
    "    return tf.matmul(inputs, W) + b\n",
    "\n",
    "# Cross-Entropy loss function with logistic resgression\n",
    "def loss_fn(linear_model, inputs, labels):\n",
    "    return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=linear_model(inputs), labels=labels))\n",
    "\n",
    "# Calculate accuracy\n",
    "def accuracy_fn(linear_model, inputs, labels, debug=False):\n",
    "    prediction = tf.nn.softmax(linear_model(inputs))\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), labels)\n",
    "    if debug:\n",
    "        print (prediction)\n",
    "        print (correct_pred)\n",
    "    return tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient descent optimizer and provide function to compute gradient\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "grad = tfe.implicit_gradients(loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "average_loss = 0.\n",
    "average_acc = 0.\n",
    "average_loss_list = []\n",
    "average_acc_list = []\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Iterate through the dataset\n",
    "    try:\n",
    "        d = dataset_iter.next()\n",
    "    except StopIteration:\n",
    "        # Refill queue\n",
    "        dataset_iter = tfe.Iterator(dataset)\n",
    "        d = dataset_iter.next()\n",
    "\n",
    "    # Images\n",
    "    x_batch = d[0]\n",
    "    # Labels\n",
    "    y_batch = tf.cast(d[1], dtype=tf.int64)\n",
    "\n",
    "    batch_loss = loss_fn(linear_combiner, x_batch, y_batch)\n",
    "    average_loss += batch_loss\n",
    "\n",
    "    batch_accuracy = accuracy_fn(linear_combiner, x_batch, y_batch)\n",
    "    average_acc += batch_accuracy\n",
    "\n",
    "    if step == 0:\n",
    "        average_loss_list.append(average_loss)\n",
    "        average_acc_list.append(average_acc)\n",
    "\n",
    "    # Update the variables (weights) based on gradients\n",
    "    optimizer.apply_gradients(grad(linear_combiner, x_batch, y_batch))\n",
    "\n",
    "    if (step + 1) % display_step == 0 or step == 0:\n",
    "        if step > 0:\n",
    "            average_loss /= display_step\n",
    "            average_acc /= display_step\n",
    "            average_loss_list.append(average_loss)\n",
    "            average_acc_list.append(average_acc)\n",
    "            print(average_acc)\n",
    "        average_loss = 0.\n",
    "        average_acc = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "average_loss_list = np.asarray(average_loss_list)\n",
    "average_acc_list = np.asarray(average_acc_list)\n",
    "plt.plot(average_acc_list,label=\"Average accuracy\")\n",
    "#plt.plot(average_loss_list,label=\"Average loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Validation accuracy\n",
    "validation_images = mnist.validation.images[0:100]\n",
    "validation_labels = mnist.validation.labels[0:100]\n",
    "#plot_img(validation_images, [1,41,71])\n",
    "#Accuracy of validation\n",
    "test_acc = accuracy_fn(linear_combiner, validation_images, validation_labels)\n",
    "print(\"Validation accuracy: {:.4f}\".format(test_acc))\n",
    "# Test set accuracy\n",
    "testX = mnist.test.images\n",
    "testY = mnist.test.labels\n",
    "test_acc = accuracy_fn(linear_combiner, testX, testY)\n",
    "print(\"Test set accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adversial examples\n",
    "validation_images += 0.007\n",
    "random_image = np.random.uniform(0,0.001,784)\n",
    "adversial_validation = np.copy(validation_images) * random_image\n",
    "adversial_validation = tf.cast(adversial_validation, tf.float32)\n",
    "plot_img(adversial_validation, [1,41,71])\n",
    "#Accuracy\n",
    "test_acc = accuracy_fn(linear_combiner, adversial_validation, validation_labels)\n",
    "print(\"Adversial validation Accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class mnistTwoClassifiers(object):\n",
    "    def __init__(self):\n",
    "        self.step_display = 100\n",
    "    \n",
    "    def update_param(self, nr_inputs, nr_classes, learning_rate, step_size):\n",
    "        self.inputs = nr_inputs\n",
    "        self.nr_classes = nr_classes\n",
    "        self.W = tfe.Variable(tf.zeros([nr_inputs, nr_classes]), name='weights')\n",
    "        self.b = tfe.Variable(tf.zeros([nr_classes]), name='biases')\n",
    "        self.learning_rate = learning_rate\n",
    "        self.step_size = step_size\n",
    "        \n",
    "        # Gradient descent optimizer and provide function to compute gradient\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        self.grad = tfe.implicit_gradients(self.logistic_loss_fn)\n",
    "        \n",
    "    def update_dataset(self, images, labels, batch_size):\n",
    "        self.dataset = tf.data.Dataset.from_tensor_slices((images, labels)).batch(batch_size)\n",
    "        #iterator dataset for each batch\n",
    "        self.dataset_iter = tfe.Iterator(self.dataset)\n",
    "    \n",
    "    #linear combiner (v)\n",
    "    def linear_combiner(self, inputs):\n",
    "        return tf.matmul(inputs, self.W) + self.b\n",
    "\n",
    "    # logistic regression\n",
    "    def logistic_loss_fn(self, linear_model, inputs, labels):\n",
    "        return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=linear_model(inputs), labels=labels))\n",
    "    \n",
    "    def logistic_regression_scikit(self, y, labels):\n",
    "        model = LogisticRegression()\n",
    "        model.fit(y, labels)\n",
    "        return model\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    def logistic_accuracy_fn(self, linear_model, inputs, labels, debug=False):\n",
    "        prediction = tf.nn.softmax(linear_model(inputs))\n",
    "        correct_pred = tf.equal(tf.argmax(prediction, 1), labels)\n",
    "        if debug:\n",
    "            print (prediction)\n",
    "            print (correct_pred)\n",
    "        return tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    def save_model(self, name):        \n",
    "        sess = tf.Session()\n",
    "        model_saver = tf.train.Saver({\"weights\": self.W, \"biases\": self.b})\n",
    "        model_saver.save(sess, \"/tmp/\"+str(name)+\".ckpt\")\n",
    "        print(\"Model saved\")\n",
    "    \n",
    "    def load_model(self, name):\n",
    "        W = tfe.Variable(tf.zeros([self.nr_inputs, self.nr_classes]), name='weights')\n",
    "        b = tfe.Variable(tf.zeros([self.nr_classes]), name='biases')\n",
    "        sess = tf.Session()\n",
    "        saver = tf.train.Saver({\"weights\": W, \"biases\": b})\n",
    "        saver.restore(sess, str(name))\n",
    "        \n",
    "        return W, b\n",
    "    \n",
    "    def train_logistic_reg(self):\n",
    "        # Training\n",
    "        average_loss = 0.\n",
    "        average_acc = 0.\n",
    "        average_loss_list = []\n",
    "        average_acc_list = []\n",
    "\n",
    "        for step_ in range(self.step_size):\n",
    "            # Iterate through the dataset\n",
    "            try:\n",
    "                d = self.dataset_iter.next()\n",
    "            except StopIteration:\n",
    "                self.dataset_iter = tfe.Iterator(self.dataset)\n",
    "                d = self.dataset_iter.next()\n",
    "            # Images and labels\n",
    "            x_batch = d[0]\n",
    "            y_batch = tf.cast(d[1], dtype=tf.int64)\n",
    "\n",
    "            batch_loss = self.logistic_loss_fn(self.linear_combiner, x_batch, y_batch)\n",
    "            average_loss += batch_loss\n",
    "\n",
    "            batch_accuracy = self.logistic_accuracy_fn(self.linear_combiner, x_batch, y_batch)\n",
    "            average_acc += batch_accuracy\n",
    "\n",
    "            if step_ == 0:\n",
    "                average_loss_list.append(average_loss)\n",
    "                average_acc_list.append(average_acc)\n",
    "\n",
    "            # Update the variables (weights) based on gradients\n",
    "            self.optimizer.apply_gradients(self.grad(self.linear_combiner, x_batch, y_batch))\n",
    "\n",
    "            if (step_ + 1) % self.step_display == 0 or step_ == 0:\n",
    "                if step_ > 0:\n",
    "                    average_loss /= self.step_display\n",
    "                    average_acc /= self.step_display\n",
    "                    average_loss_list.append(average_loss)\n",
    "                    average_acc_list.append(average_acc)\n",
    "                    print(average_acc)\n",
    "                average_loss = 0.\n",
    "                average_acc = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.8150998, shape=(), dtype=float32)\n",
      "tf.Tensor(0.89120024, shape=(), dtype=float32)\n",
      "tf.Tensor(0.8918998, shape=(), dtype=float32)\n",
      "tf.Tensor(0.9011999, shape=(), dtype=float32)\n",
      "tf.Tensor(0.9001001, shape=(), dtype=float32)\n",
      "tf.Tensor(0.9184, shape=(), dtype=float32)\n",
      "tf.Tensor(0.9080999, shape=(), dtype=float32)\n",
      "tf.Tensor(0.9146998, shape=(), dtype=float32)\n",
      "tf.Tensor(0.90470016, shape=(), dtype=float32)\n",
      "tf.Tensor(0.9118001, shape=(), dtype=float32)\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "nr_inputs = 784\n",
    "nr_classes = 10\n",
    "learning_rate = 0.5\n",
    "step_size = 1000\n",
    "train_images = mnist.train.images\n",
    "train_labels = mnist.train.labels\n",
    "batch_size = 100\n",
    "\n",
    "mnist_classifiers = mnistTwoClassifiers()\n",
    "mnist_classifiers.update_param(nr_inputs, nr_classes, learning_rate, step_size)\n",
    "mnist_classifiers.update_dataset(train_images, train_labels, batch_size)\n",
    "mnist_classifiers.train_logistic_reg()\n",
    "mnist_classifiers.save_model(\"logistic_reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_reg = LogisticRegression()\n",
    "X_test = train_images\n",
    "y_test = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 120 ms, sys: 0 ns, total: 120 ms\n",
      "Wall time: 121 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_size = 500\n",
    "X_train = train_images[0:sample_size]\n",
    "y_train = train_labels[:sample_size]\n",
    "%time logistic_reg.fit(X_train, y_train)\n",
    "#regr.score(X_test, y_test)\n",
    "logistic_reg.score(train_images[50:100], train_labels[50:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
